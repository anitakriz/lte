{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pendulum_data import get_pendulum_data\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = get_pendulum_data(100)\n",
    "val_data = get_pendulum_data(10)\n",
    "test_data = get_pendulum_data(10)\n",
    "\n",
    "# Save datasets to files\n",
    "\n",
    "# output_dir = '/usr/local/data/anitakriz/ode/lte/pendulum/data'\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# train_file = os.path.join(output_dir, 'pendulum_train.npz')\n",
    "# val_file = os.path.join(output_dir, 'pendulum_val.npz')\n",
    "# test_file = os.path.join(output_dir, 'pendulum_test.npz')\n",
    "\n",
    "# np.savez(train_file, **training_data)\n",
    "# np.savez(val_file, **val_data)\n",
    "# np.savez(test_file, **test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of visually identical images: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def count_identical_images(images):\n",
    "    \"\"\"\n",
    "    Count how many images are visually identical based on their non-zero pixel values.\n",
    "    \n",
    "    Args:\n",
    "        images (numpy.ndarray): Array of images with shape (n_samples, width, height).\n",
    "    \n",
    "    Returns:\n",
    "        int: Number of duplicate images.\n",
    "        dict: A mapping of unique pixel content to the number of occurrences.\n",
    "    \"\"\"\n",
    "    # Dictionary to store unique image representations\n",
    "    image_groups = {}\n",
    "    \n",
    "    for i, img in enumerate(images):\n",
    "        # Extract non-zero pixel values and their positions\n",
    "        non_zero_pixels = tuple(img[img != 0])\n",
    "        \n",
    "        # Use the tuple as a key for grouping identical images\n",
    "        if non_zero_pixels in image_groups:\n",
    "            image_groups[non_zero_pixels].append(i)\n",
    "        else:\n",
    "            image_groups[non_zero_pixels] = [i]\n",
    "    \n",
    "    # Count duplicates\n",
    "    duplicate_count = sum(len(indices) - 1 for indices in image_groups.values() if len(indices) > 1)\n",
    "    \n",
    "    return duplicate_count, image_groups\n",
    "\n",
    "# Example usage\n",
    "data = get_pendulum_data(n_ics=100)\n",
    "images = data['x'].reshape(-1, 51, 51)  # Reshape to (n_samples, width, height)\n",
    "\n",
    "duplicates, image_groups = count_identical_images(images)\n",
    "print(f\"Number of visually identical images: {duplicates}\")\n",
    "\n",
    "# Optional: Print details of identical groups\n",
    "for key, indices in image_groups.items():\n",
    "    if len(indices) > 1:\n",
    "        print(f\"Identical images found at indices: {indices}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4004834945136456e-80\n",
      "-7.291313035766116\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'done' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m dx_data_images \u001b[38;5;241m=\u001b[39m training_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdx\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Shape: (n_ics * n_timesteps, 51, 51)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(dx_data_images[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmin())\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdone\u001b[49m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Initialize the counter for zero derivatives\u001b[39;00m\n\u001b[1;32m     10\u001b[0m zero_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'done' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# Extract the data from training_data\n",
    "x_data_images = training_data['x']  # Shape: (n_ics * n_timesteps, 51, 51)\n",
    "print(x_data_images[0].min())\n",
    "dx_data_images = training_data['dx']  # Shape: (n_ics * n_timesteps, 51, 51)\n",
    "print(dx_data_images[0].min())\n",
    "print(done)\n",
    "# Initialize the counter for zero derivatives\n",
    "zero_count = 0\n",
    "\n",
    "# Loop over all trajectories and timesteps\n",
    "for trajectory_idx in range(dx_data_images.shape[0]):  # Iterate over trajectorie\n",
    "    if np.all(dx_data_images[trajectory_idx] == 0):\n",
    "        zero_count += 1\n",
    "\n",
    "# Print the total count of zero derivatives\n",
    "print(f\"Total number of zero derivatives (dx=0) across all trajectories and timesteps: {zero_count} out of {dx_data_images.shape[0]}\")\n",
    "\n",
    "\n",
    "ddx_data_images = training_data['ddx']  # Shape: (n_ics * n_timesteps, 51, 51)\n",
    "\n",
    "# Get the time vector and organize data\n",
    "n_ics = 100  # Number of initial conditions\n",
    "n_timesteps = len(training_data['t'])  # Number of timesteps\n",
    "t = training_data['t']  # Time vector\n",
    "sample_idx = 1  # Choose the first initial condition\n",
    "\n",
    "# Reshape the data for easier indexing: (n_ics, n_timesteps, img_size, img_size)\n",
    "x_data_images = x_data_images.reshape(n_ics, n_timesteps, 51, 51)\n",
    "dx_data_images = dx_data_images.reshape(n_ics, n_timesteps, 51, 51)\n",
    "ddx_data_images = ddx_data_images.reshape(n_ics, n_timesteps, 51, 51)\n",
    "\n",
    "# Select a few timesteps for visualization\n",
    "n_samples = 100  # Number of timesteps to visualize\n",
    "timesteps_to_show = np.linspace(0, n_timesteps - 1, n_samples, dtype=int)\n",
    "\n",
    "# Plot x, dx, and ddx for the selected timesteps\n",
    "fig, axes = plt.subplots(3, n_samples, figsize=(n_samples * 2, 9))\n",
    "titles = [\"x\", \"dx\", \"ddx\"]\n",
    "\n",
    "for row, data_images in enumerate([x_data_images, dx_data_images, ddx_data_images]):\n",
    "    for col, t_idx in enumerate(timesteps_to_show):\n",
    "        axes[row, col].imshow(data_images[sample_idx, t_idx], cmap='gray', origin='lower')\n",
    "        axes[row, col].set_title(f\"{titles[row]} at Timestep {t[t_idx]:.2f}\")\n",
    "        axes[row, col].axis('off')\n",
    "\n",
    "# plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 51, 51)\n",
      "(500,)\n"
     ]
    }
   ],
   "source": [
    "print(training_data['x'].shape)\n",
    "print(training_data['t'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/usr/local/data/anitakriz/ode/lte/pendulum/data'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "train_file = os.path.join(output_dir, 'pendulum_train.npz') \n",
    "val_file = os.path.join(output_dir, 'pendulum_val.npz') \n",
    "test_file = os.path.join(output_dir, 'pendulum_test.npz') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.   0.02 0.04 0.06 0.08 0.1  0.12 0.14 0.16 0.18 0.2  0.22 0.24 0.26\n",
      " 0.28 0.3  0.32 0.34 0.36 0.38 0.4  0.42 0.44 0.46 0.48 0.5  0.52 0.54\n",
      " 0.56 0.58 0.6  0.62 0.64 0.66 0.68 0.7  0.72 0.74 0.76 0.78 0.8  0.82\n",
      " 0.84 0.86 0.88 0.9  0.92 0.94 0.96 0.98 1.   1.02 1.04 1.06 1.08 1.1\n",
      " 1.12 1.14 1.16 1.18 1.2  1.22 1.24 1.26 1.28 1.3  1.32 1.34 1.36 1.38\n",
      " 1.4  1.42 1.44 1.46 1.48 1.5  1.52 1.54 1.56 1.58 1.6  1.62 1.64 1.66\n",
      " 1.68 1.7  1.72 1.74 1.76 1.78 1.8  1.82 1.84 1.86 1.88 1.9  1.92 1.94\n",
      " 1.96 1.98 2.   2.02 2.04 2.06 2.08 2.1  2.12 2.14 2.16 2.18 2.2  2.22\n",
      " 2.24 2.26 2.28 2.3  2.32 2.34 2.36 2.38 2.4  2.42 2.44 2.46 2.48 2.5\n",
      " 2.52 2.54 2.56 2.58 2.6  2.62 2.64 2.66 2.68 2.7  2.72 2.74 2.76 2.78\n",
      " 2.8  2.82 2.84 2.86 2.88 2.9  2.92 2.94 2.96 2.98 3.   3.02 3.04 3.06\n",
      " 3.08 3.1  3.12 3.14 3.16 3.18 3.2  3.22 3.24 3.26 3.28 3.3  3.32 3.34\n",
      " 3.36 3.38 3.4  3.42 3.44 3.46 3.48 3.5  3.52 3.54 3.56 3.58 3.6  3.62\n",
      " 3.64 3.66 3.68 3.7  3.72 3.74 3.76 3.78 3.8  3.82 3.84 3.86 3.88 3.9\n",
      " 3.92 3.94 3.96 3.98 4.   4.02 4.04 4.06 4.08 4.1  4.12 4.14 4.16 4.18\n",
      " 4.2  4.22 4.24 4.26 4.28 4.3  4.32 4.34 4.36 4.38 4.4  4.42 4.44 4.46\n",
      " 4.48 4.5  4.52 4.54 4.56 4.58 4.6  4.62 4.64 4.66 4.68 4.7  4.72 4.74\n",
      " 4.76 4.78 4.8  4.82 4.84 4.86 4.88 4.9  4.92 4.94 4.96 4.98 5.   5.02\n",
      " 5.04 5.06 5.08 5.1  5.12 5.14 5.16 5.18 5.2  5.22 5.24 5.26 5.28 5.3\n",
      " 5.32 5.34 5.36 5.38 5.4  5.42 5.44 5.46 5.48 5.5  5.52 5.54 5.56 5.58\n",
      " 5.6  5.62 5.64 5.66 5.68 5.7  5.72 5.74 5.76 5.78 5.8  5.82 5.84 5.86\n",
      " 5.88 5.9  5.92 5.94 5.96 5.98 6.   6.02 6.04 6.06 6.08 6.1  6.12 6.14\n",
      " 6.16 6.18 6.2  6.22 6.24 6.26 6.28 6.3  6.32 6.34 6.36 6.38 6.4  6.42\n",
      " 6.44 6.46 6.48 6.5  6.52 6.54 6.56 6.58 6.6  6.62 6.64 6.66 6.68 6.7\n",
      " 6.72 6.74 6.76 6.78 6.8  6.82 6.84 6.86 6.88 6.9  6.92 6.94 6.96 6.98\n",
      " 7.   7.02 7.04 7.06 7.08 7.1  7.12 7.14 7.16 7.18 7.2  7.22 7.24 7.26\n",
      " 7.28 7.3  7.32 7.34 7.36 7.38 7.4  7.42 7.44 7.46 7.48 7.5  7.52 7.54\n",
      " 7.56 7.58 7.6  7.62 7.64 7.66 7.68 7.7  7.72 7.74 7.76 7.78 7.8  7.82\n",
      " 7.84 7.86 7.88 7.9  7.92 7.94 7.96 7.98 8.   8.02 8.04 8.06 8.08 8.1\n",
      " 8.12 8.14 8.16 8.18 8.2  8.22 8.24 8.26 8.28 8.3  8.32 8.34 8.36 8.38\n",
      " 8.4  8.42 8.44 8.46 8.48 8.5  8.52 8.54 8.56 8.58 8.6  8.62 8.64 8.66\n",
      " 8.68 8.7  8.72 8.74 8.76 8.78 8.8  8.82 8.84 8.86 8.88 8.9  8.92 8.94\n",
      " 8.96 8.98 9.   9.02 9.04 9.06 9.08 9.1  9.12 9.14 9.16 9.18 9.2  9.22\n",
      " 9.24 9.26 9.28 9.3  9.32 9.34 9.36 9.38 9.4  9.42 9.44 9.46 9.48 9.5\n",
      " 9.52 9.54 9.56 9.58 9.6  9.62 9.64 9.66 9.68 9.7  9.72 9.74 9.76 9.78\n",
      " 9.8  9.82 9.84 9.86 9.88 9.9  9.92 9.94 9.96 9.98]\n"
     ]
    }
   ],
   "source": [
    "print(training_data['t'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 130050000 into shape (100,500,7,7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m n_timesteps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(t)\n\u001b[1;32m     11\u001b[0m img_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39msqrt(x_data_flat\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]))  \u001b[38;5;66;03m# Assuming square images\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m x_data_images \u001b[38;5;241m=\u001b[39m \u001b[43mx_data_flat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_ics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_timesteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Shape: (100, number_of_timesteps, 51, 51)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Visualize a few images for the first initial condition\u001b[39;00m\n\u001b[1;32m     16\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m  \u001b[38;5;66;03m# Number of timesteps to visualize\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 130050000 into shape (100,500,7,7)"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Extract the image data from training_data\n",
    "x_data_flat = training_data['x']  # Shape: (100 * number_of_timesteps, 2601)\n",
    "\n",
    "# Get the time vector and reshape the data\n",
    "n_ics = 100\n",
    "t = training_data['t']\n",
    "n_timesteps = len(t)\n",
    "img_size = int(np.sqrt(x_data_flat.shape[1]))  # Assuming square images\n",
    "\n",
    "x_data_images = x_data_flat.reshape(n_ics, n_timesteps, img_size, img_size)  # Shape: (100, number_of_timesteps, 51, 51)\n",
    "\n",
    "# Visualize a few images for the first initial condition\n",
    "n_samples = 5  # Number of timesteps to visualize\n",
    "sample_idx = 1  # Choose the first initial condition\n",
    "timesteps_to_show = np.linspace(0, n_timesteps - 1, n_samples, dtype=int)\n",
    "\n",
    "fig, axes = plt.subplots(1, n_samples, figsize=(15, 3))\n",
    "for i, t_idx in enumerate(timesteps_to_show):\n",
    "    axes[i].imshow(x_data_images[sample_idx, t_idx], cmap='viridis', origin='lower')\n",
    "    axes[i].set_title(f\"Timestep {t[t_idx]:.2f}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "\n",
    "params['input_dim'] = training_data['x'].shape[-1]\n",
    "params['latent_dim'] = 1\n",
    "params['model_order'] = 2\n",
    "params['poly_order'] = 3\n",
    "params['include_sine'] = True\n",
    "params['library_dim'] = library_size(2*params['latent_dim'], params['poly_order'], params['include_sine'], True)\n",
    "\n",
    "# sequential thresholding parameters\n",
    "params['sequential_thresholding'] = True\n",
    "params['coefficient_threshold'] = 0.1\n",
    "params['threshold_frequency'] = 500\n",
    "params['coefficient_mask'] = np.ones((params['library_dim'], params['latent_dim']))\n",
    "params['coefficient_initialization'] = 'constant'\n",
    "\n",
    "# loss function weighting\n",
    "params['loss_weight_decoder'] = 1.0\n",
    "params['loss_weight_sindy_x'] = 5e-4\n",
    "params['loss_weight_sindy_z'] = 5e-5\n",
    "params['loss_weight_sindy_regularization'] = 1e-5\n",
    "\n",
    "params['activation'] = 'sigmoid'\n",
    "params['widths'] = [128,64,32]\n",
    "\n",
    "# training parameters\n",
    "params['epoch_size'] = training_data['x'].shape[0]\n",
    "params['batch_size'] = 1000\n",
    "params['learning_rate'] = 1e-4\n",
    "\n",
    "params['data_path'] = os.getcwd() + '/'\n",
    "params['print_progress'] = True\n",
    "params['print_frequency'] = 100\n",
    "\n",
    "# training time cutoffs\n",
    "params['max_epochs'] = 5001\n",
    "params['refinement_epochs'] = 1001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Required arguments\n",
    "    parser.add_argument('--dataset_dir', type=str, required=True, help='Path to where npz files of dataset are stored')\n",
    "    parser.add_argument('--save_dir', type=str, required=True, help='Path to where you want to save experiment')\n",
    "    parser.add_argument('--device', type=int, default=0, help='GPU device')\n",
    "\n",
    "    # Training arguments\n",
    "    parser.add_argument('--epochs', type=int, default=5000, help='Training epochs')\n",
    "    parser.add_argument('--bs', type=int, default=1024, help='Training epochs')\n",
    "    parser.add_argument('--refinement_epochs', type=int, default=1000, help='Epochs during which mask is fixed and no L1 reg')\n",
    "    parser.add_argument('--lr', type=float, default=1e-4, help='Learning rate')\n",
    "    parser.add_argument('--timepoints', type=int, default= 10, help='Number of timepoints')\n",
    "    parser.add_argument('--delta_t', type=float, default= .05, help='Change in t between timepoints')\n",
    "    parser.add_argument('--pred_timepoints', type=int, default=5, help='Number of timepoints to predict in the future')\n",
    "    parser.add_argument('--input_dim', type=int, default=32, help='Input dimension')\n",
    "    parser.add_argument('--latent_ch', type=int, default=256, help='Latent Channels of Encoder')\n",
    "    parser.add_argument('--latent_dim', type=int, default=3, help='Latent dimension')\n",
    "    parser.add_argument('--library_dim', type=int, default=7, help='Number of candidate functions in the library')\n",
    "    parser.add_argument('--beta', type=float, default=.5, help='Beta term in ELBO')\n",
    "    parser.add_argument(\"--viz_freq\", help=\"Steps per visualisation.\", type=int, default=10000)\n",
    "    parser.add_argument('--supervised', action='store_true', help='Add supervised loss in latent space')\n",
    "    # Loss weights'\n",
    "    parser.add_argument('--lambda_1', type=float, default= 5e-4, help='Weight on x-level derivative loss')\n",
    "    parser.add_argument('--lambda_2', type=float, default= 5e-5, help='Weight on z-level derivative loss')\n",
    "    parser.add_argument('--lambda_3', type=float, default= 1e-5, help='Weight on regularization loss')\n",
    "    parser.add_argument('--lambda_4', type=float, default= 1e-6, help='Weight on supervised loss' )\n",
    "\n",
    "    # Evaluation and checkpoints\n",
    "    parser.add_argument('--eval_freq', type=int, default=10, help='Frequency of evaluation')\n",
    "    parser.add_argument('--best_loss', type=float, default=float('inf'), help='Initial best loss')\n",
    "\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse\n",
    "sys.path.append('../')\n",
    "from autoencoder import Autoencoder\n",
    "from train import setup_dataloader, run_epoch\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "checkpoint_path = '/usr/local/data/anitakriz/ode/lte/pendulum/exps/model_1/checkpoint.pt'\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "# Load model state_dict\n",
    "hparams = checkpoint.get('hparams', {})\n",
    "\n",
    "# Convert the hparams dictionary into a Namespace object\n",
    "args = argparse.Namespace(**hparams)\n",
    "args.num_ode = 1\n",
    "args.static = False\n",
    "model = Autoencoder(args).to(args.device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# If optimizer state was saved, load it too\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# If training state was saved\n",
    "epoch = checkpoint.get('epoch', 0)\n",
    "loss = checkpoint.get('loss', None)\n",
    "\n",
    "print(f\"Checkpoint loaded. Epoch: {epoch}, Loss: {loss}\")\n",
    "print(args.device)\n",
    "model.eval()\n",
    "_, _, test_loader = setup_dataloader(args)\n",
    "test_loss, test_individual_losses = run_epoch(args, model, test_loader, optimizer, training=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from utils import save_dynamics\n",
    "from tqdm import tqdm\n",
    "# Iterate over the data loader\n",
    "mode = \"Validation\"\n",
    "progress_bar = tqdm(test_loader, desc=f\"{mode} Epoch\", total=len(test_loader), dynamic_ncols=True)\n",
    "\n",
    "for i, batch in enumerate(progress_bar):  \n",
    "    save_dynamics(args, model, batch)\n",
    "    print(done)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ode_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
